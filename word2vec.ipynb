{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "134dQSZD37rZWLBCGRaVIBbRhzRyZAZkw",
      "authorship_tag": "ABX9TyMVMs3RvBAbW+lZr+z9rqL4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guiniel/Pasantia-IA-CNTM/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos las importaciones necesarias"
      ],
      "metadata": {
        "id": "yfEAyLCl94EI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mnBPEXc_DLH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.utils import shuffle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM,Bidirectional,Dropout, GlobalMaxPooling1D, Conv1D, AveragePooling1D\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.layers import Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos un archivo que contiene unicamente una columna, dicha columna esta compuesta de requerimientos de software\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UOiO6ve7-BFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data/reqTxt.csv', header=None)\n",
        "dfRequire = df.iloc[:,:]\n",
        "print(dfRequire.shape)\n",
        "print(dfRequire.columns)\n",
        "X = dfRequire[0]\n",
        "print(X[0])\n",
        "X = np.array(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFu2m-d3_1sf",
        "outputId": "1bd5f64f-78b9-4426-d483-a990b0ae6fe9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23313, 1)\n",
            "Int64Index([0], dtype='int64')\n",
            "add ca against object literals in function invocationsthe idea here is that if our metadata captures a type as function arg we should be able to create an instance of that type as an object literal as an arg to a function invocation for examplep    tiuicreatelabel  ltpropertycaheregt  code prediv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se carga el conjunto de datos que corresponde a los puntos de esfuerzo de las tareas que se cargaron previamente"
      ],
      "metadata": {
        "id": "PMgNNB7d-oDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X))\n",
        "y = pd.read_csv('/content/drive/MyDrive/data/estiDeep.data', header=None)\n",
        "y = np.array(y)\n",
        "print ('Shape of label tensor:', y.shape)\n",
        "print(y.dtype)\n",
        "\n",
        "\n",
        "MAX_LEN = 23313 # Cantidad total de registros\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1000)\n",
        "kf.get_n_splits(X)\n",
        "print(kf)\n",
        "print(y[:500])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjQBWxmDAiwt",
        "outputId": "5c1ce879-39a2-4f25-d1e8-66d41a66f195"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23313\n",
            "Shape of label tensor: (23313, 1)\n",
            "int64\n",
            "KFold(n_splits=10, random_state=1000, shuffle=True)\n",
            "[[ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [13]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [20]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [13]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [13]\n",
            " [13]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 3]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [21]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [40]\n",
            " [20]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 1]\n",
            " [34]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [20]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 1]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 2]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 1]\n",
            " [40]\n",
            " [ 5]\n",
            " [13]\n",
            " [40]\n",
            " [13]\n",
            " [13]\n",
            " [13]\n",
            " [13]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [13]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 8]\n",
            " [21]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 2]\n",
            " [ 2]\n",
            " [21]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 5]\n",
            " [20]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [13]\n",
            " [20]\n",
            " [ 8]\n",
            " [13]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [13]\n",
            " [13]\n",
            " [20]\n",
            " [20]\n",
            " [20]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [20]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 1]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [13]\n",
            " [13]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 2]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [20]\n",
            " [20]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 1]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [13]\n",
            " [13]\n",
            " [20]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 2]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 2]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 2]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [13]\n",
            " [20]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [13]\n",
            " [20]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 8]\n",
            " [13]\n",
            " [20]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 3]\n",
            " [ 3]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 3]\n",
            " [13]\n",
            " [ 3]\n",
            " [13]\n",
            " [13]\n",
            " [20]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 2]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [13]\n",
            " [13]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [13]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [ 8]\n",
            " [13]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]\n",
            " [ 5]\n",
            " [ 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui cargamos lo llamado embedding pre-entrenado. Esto consiste en **representaciones vectoriales de palabras o frases que se han entrenado previamente en grandes conjuntos de datos textuales**, como corpus de noticias, libros o documentos web. La idea principal detrás de los embeddings pre-entrenados es capturar el significado de las palabras en un espacio vectorial continuo donde palabras con significados similares están más cerca entre sí en este espacio."
      ],
      "metadata": {
        "id": "Kj3oFu8--4I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pre-entrenamiento\n",
        "pret_model = pd.read_csv('/content/drive/My Drive/pretrain_model/word2vec_SE.csv', sep= ',', header=None)\n"
      ],
      "metadata": {
        "id": "pTgQbB45Aojk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui entonces, creamos nuestra matriz la cual sirve de modelo preentrenado para proseguir al siguiente paso"
      ],
      "metadata": {
        "id": "hDxV56Pc_1yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = pret_model.iloc[1:,:]\n",
        "\n",
        "dfEmbedding_mat = pd.DataFrame(embedding_matrix)\n",
        "embedding_mat = dfEmbedding_mat.fillna('0')\n",
        "\n",
        "print('Embedding mat: ' + str(embedding_mat.shape))\n",
        "\n",
        "vetMAE = []\n",
        "vetR2 = []\n",
        "vetMSE = []\n",
        "vetMdae = []\n",
        "i = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwLmuAWTIO_0",
        "outputId": "40854bb7-dc24-48f8-8751-d2f7722d199f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding mat: (23313, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, test_index in kf.split(X):\n",
        "\n",
        "    x_train, test_x = X[train_index], X[test_index]\n",
        "    train_y, test_y = y[train_index], y[test_index]\n",
        "\n",
        "\n",
        "    texts_train = x_train.astype(str)\n",
        "    texts_test = test_x.astype(str)\n",
        "\n",
        "    tokenizer = Tokenizer(num_words = MAX_LEN, char_level=False, lower=False)\n",
        "    tokenizer.fit_on_texts(texts_train)\n",
        "    encSequences = tokenizer.texts_to_sequences(texts_train)\n",
        "    encSequences_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('Vocab_size: '+ str(vocab_size))\n",
        "\n",
        "    MAX_SEQUENCE_LENGTH = 100\n",
        "\n",
        "    x_train = pad_sequences(encSequences, maxlen= MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    x_test = pad_sequences(encSequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    print('Shape of data tensor:', x_train.shape)\n",
        "    print('Shape of data test tensor:', x_test.shape)\n",
        "\n",
        "    print('train_y: ' + str(train_y.shape))\n",
        "    print('test_y: ' + str(test_y.shape))\n",
        "\n",
        "\n",
        "    #Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding = Embedding(MAX_LEN, 100, input_length = 100, name='embedding')\n",
        "    embedding.build(input_shape=(1,))\n",
        "    embedding.set_weights([embedding_mat])\n",
        "    model.add(embedding)\n",
        "\n",
        "    model.add(AveragePooling1D(pool_size=100))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    adam = Adam(lr = 0.001, beta_1 = 0.99, beta_2 = 0.999, epsilon = None, amsgrad = False)\n",
        "\n",
        "    model.compile(loss = 'mse', optimizer= 'adam', metrics=['mae'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    print('Modelo compilado...')\n",
        "\n",
        "    es = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
        "\n",
        "    model_history = model.fit(x_train, train_y,\n",
        "              batch_size= 128,\n",
        "              epochs=30, callbacks=[es],\n",
        "              validation_data=(x_test, test_y))\n",
        "\n",
        "    y_pred = model.predict(x_test, batch_size=None, verbose=0, steps=None)\n",
        "    x_pred = model.predict(x_train, batch_size=None, verbose=0, steps=None)\n",
        "\n",
        "\n",
        "    #Metricas\n",
        "    print(\"\\n\")\n",
        "    mae = mean_absolute_error(test_y, y_pred)\n",
        "    vetMAE.append(mae)\n",
        "    print(\"MAE: %f\" % (mae))\n",
        "    medAE = median_absolute_error(test_y, y_pred)\n",
        "    vetMdae.append(medAE)\n",
        "    print(\"MedAE: %f\" % (medAE))\n",
        "    r2 = r2_score(test_y, y_pred, multioutput='raw_values')\n",
        "    vetR2.append(r2)\n",
        "    print(\"r2: %f\" % (r2))\n",
        "    mse = mean_squared_error(test_y, y_pred)\n",
        "    vetMSE.append(mse)\n",
        "    print(\"MSE: %f\" % (mse))\n",
        "    mErr = max_error(test_y, y_pred)\n",
        "    print(\"maxrror: %f\" % (mErr))\n",
        "\n",
        "    i = i + 1\n",
        "    print(\"Final \" + str(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRCN6dleIUV9",
        "outputId": "ec19da2d-bfa6-4924-b812-ad18ee24b0fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89802 unique tokens.\n",
            "Vocab_size: 89803\n",
            "Shape of data tensor: (20981, 100)\n",
            "Shape of data test tensor: (2332, 100)\n",
            "train_y: (20981, 1)\n",
            "test_y: (2332, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d (Average  (None, 1, 100)            0         \n",
            " Pooling1D)                                                      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 41ms/step - loss: 109.2012 - mae: 4.9390 - val_loss: 93.5374 - val_mae: 4.7494\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 100.0360 - mae: 4.8279 - val_loss: 92.5988 - val_mae: 4.6123\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 98.1591 - mae: 4.7471 - val_loss: 90.4364 - val_mae: 4.4138\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 93.0640 - mae: 4.5672 - val_loss: 85.5930 - val_mae: 4.2983\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 84.9121 - mae: 4.3978 - val_loss: 81.3849 - val_mae: 4.5424\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 76.9992 - mae: 4.2141 - val_loss: 77.3110 - val_mae: 4.4152\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 69.2894 - mae: 4.0105 - val_loss: 75.0481 - val_mae: 4.1499\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 62.8526 - mae: 3.8788 - val_loss: 73.0618 - val_mae: 4.1677\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 56.9243 - mae: 3.6637 - val_loss: 71.5239 - val_mae: 4.3262\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 50.7641 - mae: 3.4972 - val_loss: 71.0977 - val_mae: 3.8748\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 45.8944 - mae: 3.3709 - val_loss: 73.0716 - val_mae: 4.6201\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 41.6193 - mae: 3.2642 - val_loss: 71.0972 - val_mae: 3.9780\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 37.6820 - mae: 3.1479 - val_loss: 71.1270 - val_mae: 4.2738\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 34.8281 - mae: 3.0785 - val_loss: 71.3915 - val_mae: 4.1974\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 9s 58ms/step - loss: 31.9773 - mae: 2.9848 - val_loss: 71.3597 - val_mae: 4.1468\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 30.0329 - mae: 2.9427 - val_loss: 72.0769 - val_mae: 3.9732\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 27.7593 - mae: 2.8380 - val_loss: 72.8554 - val_mae: 4.0932\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 25.7964 - mae: 2.7775 - val_loss: 73.3653 - val_mae: 4.1599\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 24.2008 - mae: 2.7106 - val_loss: 74.6674 - val_mae: 4.1396\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 22.5373 - mae: 2.6378Restoring model weights from the end of the best epoch: 10.\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 22.5373 - mae: 2.6378 - val_loss: 75.1326 - val_mae: 4.1378\n",
            "Epoch 20: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.874753\n",
            "MedAE: 2.085120\n",
            "r2: 0.241856\n",
            "MSE: 71.097736\n",
            "maxrror: 93.499211\n",
            "Final 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89519 unique tokens.\n",
            "Vocab_size: 89520\n",
            "Shape of data tensor: (20981, 100)\n",
            "Shape of data test tensor: (2332, 100)\n",
            "train_y: (20981, 1)\n",
            "test_y: (2332, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_1 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 13s 72ms/step - loss: 104.2831 - mae: 4.8038 - val_loss: 106.3540 - val_mae: 4.8144\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 98.6533 - mae: 4.7996 - val_loss: 105.0686 - val_mae: 4.7491\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 96.8450 - mae: 4.7370 - val_loss: 102.1165 - val_mae: 4.7432\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 91.4978 - mae: 4.5355 - val_loss: 95.3787 - val_mae: 4.6129\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 82.7014 - mae: 4.3451 - val_loss: 89.9055 - val_mae: 4.2447\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 74.9471 - mae: 4.1750 - val_loss: 85.3853 - val_mae: 4.4878\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 67.4234 - mae: 3.9751 - val_loss: 83.6581 - val_mae: 4.1410\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 60.3989 - mae: 3.7438 - val_loss: 85.8102 - val_mae: 4.8628\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 54.2346 - mae: 3.5732 - val_loss: 83.6757 - val_mae: 4.5596\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 48.4745 - mae: 3.3989 - val_loss: 82.5561 - val_mae: 4.0001\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 43.4473 - mae: 3.2468 - val_loss: 85.2867 - val_mae: 4.1639\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 39.4968 - mae: 3.1415 - val_loss: 90.6516 - val_mae: 4.2486\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 35.8564 - mae: 3.0217 - val_loss: 92.8841 - val_mae: 4.6241\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 32.1974 - mae: 2.8770 - val_loss: 98.9475 - val_mae: 4.3253\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 29.7461 - mae: 2.7966 - val_loss: 104.0836 - val_mae: 4.3419\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 27.0671 - mae: 2.6723 - val_loss: 108.2374 - val_mae: 4.2972\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 25.4571 - mae: 2.6579 - val_loss: 113.3657 - val_mae: 4.3998\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 23.0978 - mae: 2.5126 - val_loss: 118.2180 - val_mae: 4.1798\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 21.5403 - mae: 2.4288 - val_loss: 125.8316 - val_mae: 4.2928\n",
            "Epoch 20/30\n",
            "163/164 [============================>.] - ETA: 0s - loss: 19.9178 - mae: 2.3623Restoring model weights from the end of the best epoch: 10.\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 20.1061 - mae: 2.3657 - val_loss: 142.1314 - val_mae: 4.9225\n",
            "Epoch 20: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.000107\n",
            "MedAE: 2.152287\n",
            "r2: 0.225749\n",
            "MSE: 82.556130\n",
            "maxrror: 108.422050\n",
            "Final 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90065 unique tokens.\n",
            "Vocab_size: 90066\n",
            "Shape of data tensor: (20981, 100)\n",
            "Shape of data test tensor: (2332, 100)\n",
            "train_y: (20981, 1)\n",
            "test_y: (2332, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_2 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 9s 47ms/step - loss: 103.6351 - mae: 4.8439 - val_loss: 98.7063 - val_mae: 4.9210\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 99.2502 - mae: 4.7866 - val_loss: 97.3022 - val_mae: 4.7961\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 96.7417 - mae: 4.7070 - val_loss: 92.9234 - val_mae: 4.5112\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 88.8285 - mae: 4.4734 - val_loss: 83.9869 - val_mae: 4.7687\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 78.5774 - mae: 4.2189 - val_loss: 76.1395 - val_mae: 4.2674\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 69.7202 - mae: 3.9805 - val_loss: 72.0273 - val_mae: 4.0339\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 61.6007 - mae: 3.7629 - val_loss: 68.6802 - val_mae: 3.9534\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 54.6100 - mae: 3.5926 - val_loss: 67.7056 - val_mae: 4.3957\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 9s 54ms/step - loss: 48.2959 - mae: 3.4057 - val_loss: 65.3382 - val_mae: 3.9332\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 42.5507 - mae: 3.2621 - val_loss: 65.9671 - val_mae: 4.3126\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 38.6834 - mae: 3.1741 - val_loss: 66.2614 - val_mae: 3.7673\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 46ms/step - loss: 34.8691 - mae: 3.0397 - val_loss: 66.6507 - val_mae: 4.2682\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 31.7197 - mae: 2.9451 - val_loss: 71.4220 - val_mae: 4.5616\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 29.1561 - mae: 2.8649 - val_loss: 66.0104 - val_mae: 3.9799\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 26.5394 - mae: 2.7535 - val_loss: 66.9578 - val_mae: 4.0564\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 9s 57ms/step - loss: 24.3071 - mae: 2.6832 - val_loss: 65.2484 - val_mae: 3.7860\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 22.8689 - mae: 2.6269 - val_loss: 66.5991 - val_mae: 3.7918\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 21.0318 - mae: 2.5391 - val_loss: 68.9516 - val_mae: 4.1806\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 20.5343 - mae: 2.5211 - val_loss: 66.1942 - val_mae: 3.8476\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 18.7878 - mae: 2.4113 - val_loss: 70.2352 - val_mae: 4.0323\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 17.7810 - mae: 2.3592Restoring model weights from the end of the best epoch: 11.\n",
            "164/164 [==============================] - 9s 55ms/step - loss: 17.7810 - mae: 2.3592 - val_loss: 73.5590 - val_mae: 4.2803\n",
            "Epoch 21: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.767348\n",
            "MedAE: 1.952353\n",
            "r2: 0.332206\n",
            "MSE: 66.261433\n",
            "maxrror: 87.639610\n",
            "Final 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90531 unique tokens.\n",
            "Vocab_size: 90532\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_3 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 10s 51ms/step - loss: 104.6875 - mae: 4.7862 - val_loss: 108.6801 - val_mae: 5.0593\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 98.1356 - mae: 4.7685 - val_loss: 107.5937 - val_mae: 4.9648\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 96.0877 - mae: 4.6916 - val_loss: 104.8280 - val_mae: 4.7759\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 90.6570 - mae: 4.4954 - val_loss: 97.6506 - val_mae: 5.1035\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 81.4776 - mae: 4.2755 - val_loss: 89.1941 - val_mae: 4.7351\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 71.4728 - mae: 4.0021 - val_loss: 83.4196 - val_mae: 4.7382\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 62.4798 - mae: 3.7541 - val_loss: 78.9581 - val_mae: 4.5137\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 55.2148 - mae: 3.5839 - val_loss: 77.8332 - val_mae: 4.4013\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 48.3809 - mae: 3.4230 - val_loss: 76.0287 - val_mae: 4.1516\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 42.9487 - mae: 3.2833 - val_loss: 78.0796 - val_mae: 4.4937\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 38.2215 - mae: 3.1577 - val_loss: 78.9636 - val_mae: 4.3424\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 34.3718 - mae: 3.0465 - val_loss: 78.6152 - val_mae: 4.1777\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 31.3279 - mae: 2.9503 - val_loss: 83.2951 - val_mae: 4.5934\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 28.9798 - mae: 2.8748 - val_loss: 82.1093 - val_mae: 4.4727\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 26.7491 - mae: 2.7976 - val_loss: 82.4808 - val_mae: 4.4225\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 24.7002 - mae: 2.7328 - val_loss: 83.5301 - val_mae: 4.3059\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 9s 56ms/step - loss: 22.8233 - mae: 2.6372 - val_loss: 85.5398 - val_mae: 4.4907\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 21.3622 - mae: 2.5887 - val_loss: 83.8380 - val_mae: 4.2650\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 20.2010 - mae: 2.5291Restoring model weights from the end of the best epoch: 9.\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 20.2010 - mae: 2.5291 - val_loss: 86.4174 - val_mae: 4.4126\n",
            "Epoch 19: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.151625\n",
            "MedAE: 2.133861\n",
            "r2: 0.303074\n",
            "MSE: 76.028753\n",
            "maxrror: 94.938782\n",
            "Final 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89717 unique tokens.\n",
            "Vocab_size: 89718\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_4 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 10s 54ms/step - loss: 107.4459 - mae: 4.8554 - val_loss: 95.3557 - val_mae: 4.7034\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 99.7958 - mae: 4.8173 - val_loss: 94.3300 - val_mae: 4.7550\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 98.0713 - mae: 4.7551 - val_loss: 92.2997 - val_mae: 4.7147\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 92.3460 - mae: 4.5462 - val_loss: 85.7617 - val_mae: 4.2711\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 81.7875 - mae: 4.2833 - val_loss: 81.4437 - val_mae: 4.5588\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 72.3130 - mae: 4.0461 - val_loss: 78.7215 - val_mae: 4.4267\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 64.6692 - mae: 3.8514 - val_loss: 76.9557 - val_mae: 4.0022\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 57.5976 - mae: 3.6586 - val_loss: 75.9634 - val_mae: 4.2386\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 51.6400 - mae: 3.5082 - val_loss: 75.6648 - val_mae: 3.9516\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 46.2257 - mae: 3.3479 - val_loss: 78.2288 - val_mae: 4.4071\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 9s 56ms/step - loss: 41.9643 - mae: 3.2257 - val_loss: 75.7236 - val_mae: 4.0818\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 10s 58ms/step - loss: 37.7278 - mae: 3.1020 - val_loss: 77.2471 - val_mae: 4.2066\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 33.9048 - mae: 2.9565 - val_loss: 76.8108 - val_mae: 4.0242\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 31.0205 - mae: 2.8625 - val_loss: 79.3641 - val_mae: 4.1988\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 28.5732 - mae: 2.7669 - val_loss: 78.7675 - val_mae: 4.0412\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 26.4492 - mae: 2.6872 - val_loss: 80.3917 - val_mae: 4.3267\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 24.5568 - mae: 2.6120 - val_loss: 83.1808 - val_mae: 4.3350\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 22.7963 - mae: 2.5337 - val_loss: 79.4212 - val_mae: 4.0658\n",
            "Epoch 19/30\n",
            "163/164 [============================>.] - ETA: 0s - loss: 21.0705 - mae: 2.4374Restoring model weights from the end of the best epoch: 9.\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 21.1518 - mae: 2.4404 - val_loss: 88.2469 - val_mae: 4.4185\n",
            "Epoch 19: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.951613\n",
            "MedAE: 2.079866\n",
            "r2: 0.209409\n",
            "MSE: 75.664804\n",
            "maxrror: 95.052565\n",
            "Final 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89516 unique tokens.\n",
            "Vocab_size: 89517\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_5 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 45ms/step - loss: 100.3272 - mae: 4.8093 - val_loss: 115.4927 - val_mae: 5.3355\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 96.8189 - mae: 4.7398 - val_loss: 112.5963 - val_mae: 5.0408\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 91.5400 - mae: 4.5597 - val_loss: 104.8536 - val_mae: 4.6231\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 82.1324 - mae: 4.3283 - val_loss: 99.0463 - val_mae: 4.5627\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 73.5577 - mae: 4.1307 - val_loss: 97.9575 - val_mae: 4.4916\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 65.4046 - mae: 3.9042 - val_loss: 90.9610 - val_mae: 4.4172\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 58.1124 - mae: 3.6867 - val_loss: 88.9154 - val_mae: 4.3337\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 51.0885 - mae: 3.4903 - val_loss: 89.4534 - val_mae: 4.3070\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 45.0114 - mae: 3.3239 - val_loss: 91.0680 - val_mae: 4.9662\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 39.9131 - mae: 3.2197 - val_loss: 90.5995 - val_mae: 4.4390\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 35.5318 - mae: 3.0560 - val_loss: 93.3456 - val_mae: 4.3971\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 32.2802 - mae: 2.9635 - val_loss: 93.1141 - val_mae: 4.4775\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 29.2573 - mae: 2.8577 - val_loss: 100.9931 - val_mae: 4.9944\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 26.6074 - mae: 2.7759 - val_loss: 97.2652 - val_mae: 4.6541\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 24.5336 - mae: 2.6916 - val_loss: 99.6029 - val_mae: 4.6876\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 22.7044 - mae: 2.6138 - val_loss: 97.9610 - val_mae: 4.3132\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 21.3268 - mae: 2.5605 - val_loss: 104.0974 - val_mae: 4.4979\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 20.1887 - mae: 2.4986Restoring model weights from the end of the best epoch: 8.\n",
            "164/164 [==============================] - 7s 46ms/step - loss: 20.1887 - mae: 2.4986 - val_loss: 103.1146 - val_mae: 4.6590\n",
            "Epoch 18: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.307034\n",
            "MedAE: 2.153497\n",
            "r2: 0.228840\n",
            "MSE: 89.453421\n",
            "maxrror: 90.962398\n",
            "Final 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89090 unique tokens.\n",
            "Vocab_size: 89091\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_6 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 45ms/step - loss: 104.3683 - mae: 4.8816 - val_loss: 109.2871 - val_mae: 4.9722\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 98.0409 - mae: 4.7521 - val_loss: 108.0892 - val_mae: 5.1316\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 94.9257 - mae: 4.6556 - val_loss: 104.1240 - val_mae: 4.8416\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 86.7465 - mae: 4.4146 - val_loss: 99.8863 - val_mae: 4.5369\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 77.3135 - mae: 4.2298 - val_loss: 95.6063 - val_mae: 4.6847\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 69.4728 - mae: 4.0845 - val_loss: 93.3162 - val_mae: 4.6371\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 62.0023 - mae: 3.8372 - val_loss: 91.7759 - val_mae: 4.6054\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 46ms/step - loss: 55.3099 - mae: 3.6570 - val_loss: 90.5651 - val_mae: 4.6689\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 49.3702 - mae: 3.4993 - val_loss: 90.3393 - val_mae: 4.6479\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 44.2365 - mae: 3.3689 - val_loss: 89.9223 - val_mae: 4.8513\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 39.4510 - mae: 3.2073 - val_loss: 90.5148 - val_mae: 4.5309\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 35.4951 - mae: 3.0838 - val_loss: 90.3331 - val_mae: 4.3431\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 32.2205 - mae: 2.9504 - val_loss: 91.2661 - val_mae: 4.5424\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 28.7725 - mae: 2.8070 - val_loss: 92.1298 - val_mae: 4.2041\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 26.4943 - mae: 2.7203 - val_loss: 92.8096 - val_mae: 4.3375\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 24.5769 - mae: 2.6505 - val_loss: 96.0826 - val_mae: 4.4399\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 22.8950 - mae: 2.5716 - val_loss: 96.0206 - val_mae: 4.3210\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 21.2446 - mae: 2.4993 - val_loss: 100.5214 - val_mae: 4.5867\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 19.6365 - mae: 2.4028 - val_loss: 104.4106 - val_mae: 5.0110\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 18.3876 - mae: 2.3359 - val_loss: 101.5563 - val_mae: 4.3683\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 17.4623 - mae: 2.2960 - val_loss: 100.7104 - val_mae: 4.3142\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 16.2637 - mae: 2.2075 - val_loss: 107.5166 - val_mae: 4.7903\n",
            "Epoch 23/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 15.3050 - mae: 2.1440 - val_loss: 104.2095 - val_mae: 4.4118\n",
            "Epoch 24/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 14.8693 - mae: 2.1421Restoring model weights from the end of the best epoch: 14.\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 14.8693 - mae: 2.1421 - val_loss: 108.3124 - val_mae: 4.4836\n",
            "Epoch 24: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.204131\n",
            "MedAE: 1.920964\n",
            "r2: 0.159833\n",
            "MSE: 92.129778\n",
            "maxrror: 94.695519\n",
            "Final 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89905 unique tokens.\n",
            "Vocab_size: 89906\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_7 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 10s 53ms/step - loss: 103.4644 - mae: 4.8448 - val_loss: 97.2176 - val_mae: 4.8854\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 99.1039 - mae: 4.7976 - val_loss: 95.4831 - val_mae: 4.8153\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 94.9097 - mae: 4.6469 - val_loss: 90.0036 - val_mae: 4.2768\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 85.0598 - mae: 4.3838 - val_loss: 82.8520 - val_mae: 4.4274\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 74.9766 - mae: 4.1461 - val_loss: 78.2756 - val_mae: 4.0747\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 66.1203 - mae: 3.8793 - val_loss: 76.7703 - val_mae: 4.5659\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 57.7364 - mae: 3.6653 - val_loss: 74.2988 - val_mae: 4.2264\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 50.2135 - mae: 3.4869 - val_loss: 75.6122 - val_mae: 4.3291\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 44.4054 - mae: 3.3422 - val_loss: 73.4801 - val_mae: 3.9390\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 9s 54ms/step - loss: 39.8836 - mae: 3.2463 - val_loss: 75.2766 - val_mae: 4.0921\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 35.2500 - mae: 3.1049 - val_loss: 76.0825 - val_mae: 4.0432\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 31.5079 - mae: 2.9867 - val_loss: 77.3490 - val_mae: 3.9848\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 28.5650 - mae: 2.8814 - val_loss: 83.7640 - val_mae: 4.3913\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 26.1414 - mae: 2.8032 - val_loss: 82.4685 - val_mae: 4.2068\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 14s 88ms/step - loss: 24.3463 - mae: 2.7478 - val_loss: 81.5332 - val_mae: 3.9504\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 22.9768 - mae: 2.6794 - val_loss: 89.1778 - val_mae: 4.4179\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 9s 56ms/step - loss: 20.9142 - mae: 2.6040 - val_loss: 86.3024 - val_mae: 4.1346\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 19.2164 - mae: 2.5195 - val_loss: 88.2352 - val_mae: 4.1870\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 18.2501 - mae: 2.4736Restoring model weights from the end of the best epoch: 9.\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 18.2501 - mae: 2.4736 - val_loss: 88.5125 - val_mae: 4.0784\n",
            "Epoch 19: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.938954\n",
            "MedAE: 2.161639\n",
            "r2: 0.247044\n",
            "MSE: 73.480132\n",
            "maxrror: 96.040487\n",
            "Final 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89590 unique tokens.\n",
            "Vocab_size: 89591\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_8 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 9s 47ms/step - loss: 106.8682 - mae: 4.9278 - val_loss: 78.6140 - val_mae: 4.6204\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 101.1868 - mae: 4.8387 - val_loss: 77.2689 - val_mae: 4.5650\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 97.7692 - mae: 4.6751 - val_loss: 76.2278 - val_mae: 5.0547\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 89.2292 - mae: 4.4753 - val_loss: 67.9630 - val_mae: 4.1352\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 78.9005 - mae: 4.2020 - val_loss: 65.1304 - val_mae: 4.2511\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 69.2728 - mae: 3.9675 - val_loss: 67.1213 - val_mae: 4.7798\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 61.2976 - mae: 3.7726 - val_loss: 63.7536 - val_mae: 4.2671\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 54.3931 - mae: 3.6087 - val_loss: 62.3173 - val_mae: 3.9385\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 47.8121 - mae: 3.4242 - val_loss: 64.2726 - val_mae: 4.3331\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 42.9449 - mae: 3.3064 - val_loss: 62.9602 - val_mae: 3.9350\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 37.9303 - mae: 3.1402 - val_loss: 64.4160 - val_mae: 3.9939\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 34.3136 - mae: 3.0451 - val_loss: 66.7362 - val_mae: 4.0385\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 31.3367 - mae: 2.9441 - val_loss: 67.3683 - val_mae: 4.0339\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 28.8508 - mae: 2.8583 - val_loss: 79.0224 - val_mae: 4.8006\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 27.1039 - mae: 2.8136 - val_loss: 71.4165 - val_mae: 4.1131\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 24.7181 - mae: 2.7264 - val_loss: 73.9225 - val_mae: 4.1225\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 22.7192 - mae: 2.6197 - val_loss: 75.9578 - val_mae: 4.3011\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 21.3507 - mae: 2.5742 - val_loss: 81.7220 - val_mae: 4.4913\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 20.2907 - mae: 2.5201 - val_loss: 80.8430 - val_mae: 4.4535\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 19.4064 - mae: 2.4981Restoring model weights from the end of the best epoch: 10.\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 19.4064 - mae: 2.4981 - val_loss: 83.2856 - val_mae: 4.3758\n",
            "Epoch 20: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.935045\n",
            "MedAE: 2.129486\n",
            "r2: 0.202169\n",
            "MSE: 62.960151\n",
            "maxrror: 94.606740\n",
            "Final 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89471 unique tokens.\n",
            "Vocab_size: 89472\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_9 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 108.1964 - mae: 4.9316 - val_loss: 94.5679 - val_mae: 4.8025\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 99.7027 - mae: 4.8194 - val_loss: 93.7759 - val_mae: 4.7817\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 97.6829 - mae: 4.7451 - val_loss: 92.7017 - val_mae: 4.8868\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 92.6418 - mae: 4.6092 - val_loss: 88.7031 - val_mae: 4.4348\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 84.0661 - mae: 4.3960 - val_loss: 85.0608 - val_mae: 4.1873\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 74.6153 - mae: 4.0884 - val_loss: 81.0443 - val_mae: 3.9812\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 65.8499 - mae: 3.8523 - val_loss: 80.0995 - val_mae: 4.2695\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 58.6105 - mae: 3.6710 - val_loss: 78.0874 - val_mae: 3.9166\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 52.2584 - mae: 3.5322 - val_loss: 77.3540 - val_mae: 3.9205\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 46.6930 - mae: 3.3765 - val_loss: 76.1133 - val_mae: 3.8242\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 7s 46ms/step - loss: 41.8754 - mae: 3.2564 - val_loss: 76.2274 - val_mae: 4.1653\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 38.0647 - mae: 3.1501 - val_loss: 74.6366 - val_mae: 4.0860\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 35.2400 - mae: 3.0795 - val_loss: 75.1603 - val_mae: 3.8023\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 31.9893 - mae: 2.9671 - val_loss: 74.8847 - val_mae: 3.8197\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 29.9691 - mae: 2.9062 - val_loss: 76.2689 - val_mae: 4.1551\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 27.4376 - mae: 2.8254 - val_loss: 77.8342 - val_mae: 4.1070\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 25.3699 - mae: 2.7509 - val_loss: 77.8466 - val_mae: 4.0352\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 23.8269 - mae: 2.6787 - val_loss: 77.3959 - val_mae: 3.8244\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 22.8450 - mae: 2.6346 - val_loss: 79.5988 - val_mae: 4.1344\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 21.1337 - mae: 2.5663 - val_loss: 79.4193 - val_mae: 4.0542\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 20.2074 - mae: 2.5118 - val_loss: 85.6237 - val_mae: 4.4244\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 19.2494 - mae: 2.4843 - val_loss: 79.1229 - val_mae: 3.8946\n",
            "Epoch 23/30\n",
            "163/164 [============================>.] - ETA: 0s - loss: 18.1905 - mae: 2.4229Restoring model weights from the end of the best epoch: 13.\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 18.2496 - mae: 2.4245 - val_loss: 80.2140 - val_mae: 3.9613\n",
            "Epoch 23: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.802251\n",
            "MedAE: 1.988420\n",
            "r2: 0.206006\n",
            "MSE: 75.160276\n",
            "maxrror: 95.505158\n",
            "Final 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maeMedio = np.mean(vetMAE)\n",
        "madAEMedio = np.mean(vetMdae)\n",
        "r2Medio = np.mean(vetR2)\n",
        "mseMedio = np.mean(vetMSE)\n",
        "stdMae = np.std(vetMAE)\n",
        "stdr2 = np.std(vetR2)\n",
        "stdMse = np.std(vetMSE)\n",
        "\n",
        "print('maeMedio: ' + str(maeMedio))\n",
        "print('madAEMedio: ' + str(madAEMedio))\n",
        "print('r2Medio: ' + str(r2Medio))\n",
        "print('mseMedio: ' + str(mseMedio))\n",
        "print('stdMae: ' + str(stdMae))\n",
        "print('stdr2: ' + str(stdr2))\n",
        "print('stdMse: ' + str(stdMse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naaglC_Jazuw",
        "outputId": "54efb2ad-9afc-4eee-8af8-7ae599e28ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maeMedio: 3.9932860960588137\n",
            "madAEMedio: 2.075749483704567\n",
            "r2Medio: 0.2356186116480537\n",
            "mseMedio: 76.47926138071598\n",
            "stdMae: 0.1666385134552769\n",
            "stdr2: 0.04755886124402499\n",
            "stdMse: 8.820021580146916\n"
          ]
        }
      ]
    }
  ]
}