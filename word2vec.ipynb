{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "134dQSZD37rZWLBCGRaVIBbRhzRyZAZkw",
      "authorship_tag": "ABX9TyPv67aRKtGuWOrAP86by4Ee",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Guiniel/Pasantia-IA-CNTM/blob/main/word2vec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Realizamos las importaciones necesarias"
      ],
      "metadata": {
        "id": "yfEAyLCl94EI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3mnBPEXc_DLH"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from sklearn.utils import shuffle\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM,Bidirectional,Dropout, GlobalMaxPooling1D, Conv1D, AveragePooling1D\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from numpy import array\n",
        "from numpy import asarray\n",
        "from numpy import zeros\n",
        "from keras.layers import Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn import preprocessing\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos un archivo que contiene unicamente una columna, dicha columna esta compuesta de requerimientos de software\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "UOiO6ve7-BFh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/data/reqTxt.csv', header=None)\n",
        "dfRequire = df.iloc[:,:]\n",
        "print(dfRequire.shape)\n",
        "print(dfRequire.columns)\n",
        "X = dfRequire[0]\n",
        "print(X[0])\n",
        "X = np.array(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFu2m-d3_1sf",
        "outputId": "03d1abf9-df92-4241-f1a2-3bfcd2d8966b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(23313, 1)\n",
            "Int64Index([0], dtype='int64')\n",
            "add ca against object literals in function invocationsthe idea here is that if our metadata captures a type as function arg we should be able to create an instance of that type as an object literal as an arg to a function invocation for examplep    tiuicreatelabel  ltpropertycaheregt  code prediv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui se carga el conjunto de datos que corresponde a los puntos de esfuerzo de las tareas que se cargaron previamente"
      ],
      "metadata": {
        "id": "PMgNNB7d-oDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(X))\n",
        "\n",
        "print('Train and test dataset loaded...')\n",
        "\n",
        "y = pd.read_csv('/content/drive/MyDrive/data/estiDeep.data', header=None) #File containing the set of training and test labels.\n",
        "y = np.array(y)\n",
        "print ('Shape of label tensor:', y.shape)\n",
        "print(y.dtype)\n",
        "\n",
        "#Number of texts in train and test dataset\n",
        "MAX_LEN = 23313\n",
        "\n",
        "kf = KFold(n_splits=10, shuffle=True, random_state=1000)\n",
        "kf.get_n_splits(X) # returns the number of splitting iterations in the cross-validator\n",
        "print(kf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hjQBWxmDAiwt",
        "outputId": "f08386a9-6763-4084-e41a-24e714dd1be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "23313\n",
            "Train and test dataset loaded...\n",
            "Shape of label tensor: (23313, 1)\n",
            "int64\n",
            "KFold(n_splits=10, random_state=1000, shuffle=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui cargamos lo llamado embedding pre-entrenado. Esto consiste en **representaciones vectoriales de palabras o frases que se han entrenado previamente en grandes conjuntos de datos textuales**, como corpus de noticias, libros o documentos web. La idea principal detrás de los embeddings pre-entrenados es capturar el significado de las palabras en un espacio vectorial continuo donde palabras con significados similares están más cerca entre sí en este espacio."
      ],
      "metadata": {
        "id": "Kj3oFu8--4I5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargamos el pre-entrenamiento\n",
        "pret_model = pd.read_csv('/content/drive/My Drive/pretrain_model/word2vec_SE.csv', sep= ',', header=None)\n"
      ],
      "metadata": {
        "id": "pTgQbB45Aojk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui entonces, creamos nuestra matriz la cual sirve de modelo preentrenado para proseguir al siguiente paso"
      ],
      "metadata": {
        "id": "hDxV56Pc_1yQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix = pret_model.iloc[1:,:]\n",
        "\n",
        "dfEmbedding_mat = pd.DataFrame(embedding_matrix)\n",
        "embedding_mat = dfEmbedding_mat.fillna('0')\n",
        "\n",
        "print('Embedding mat: ' + str(embedding_mat.shape))\n",
        "\n",
        "vetMAE = []\n",
        "vetR2 = []\n",
        "vetMSE = []\n",
        "vetMdae = []\n",
        "i = 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bwLmuAWTIO_0",
        "outputId": "d3379a05-3a4c-48cf-d999-de3740ed2554"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding mat: (23313, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for train_index, test_index in kf.split(X):\n",
        "\n",
        "    x_train, test_x = X[train_index], X[test_index]\n",
        "    train_y, test_y = y[train_index], y[test_index]\n",
        "\n",
        "    # get the raw text data\n",
        "    texts_train = x_train.astype(str)\n",
        "    texts_test = test_x.astype(str)\n",
        "\n",
        "    # vectorize the text samples\n",
        "    tokenizer = Tokenizer(num_words = MAX_LEN, char_level=False, lower=False)\n",
        "    tokenizer.fit_on_texts(texts_train)\n",
        "    encSequences = tokenizer.texts_to_sequences(texts_train)\n",
        "    encSequences_test = tokenizer.texts_to_sequences(texts_test)\n",
        "\n",
        "    word_index = tokenizer.word_index\n",
        "    print('Found %s unique tokens.' % len(word_index))\n",
        "\n",
        "    vocab_size = len(tokenizer.word_index) + 1\n",
        "    print('Vocab_size: '+ str(vocab_size))\n",
        "\n",
        "    MAX_SEQUENCE_LENGTH = 100 #number of words in each text\n",
        "\n",
        "    x_train = pad_sequences(encSequences, maxlen= MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    x_test = pad_sequences(encSequences_test, maxlen=MAX_SEQUENCE_LENGTH, padding='post')\n",
        "    print('Shape of data tensor:', x_train.shape)\n",
        "    print('Shape of data test tensor:', x_test.shape)\n",
        "\n",
        "    print('train_y: ' + str(train_y.shape))\n",
        "    print('test_y: ' + str(test_y.shape))\n",
        "\n",
        "\n",
        "    #Sequential model\n",
        "    model = Sequential()\n",
        "\n",
        "    embedding = Embedding(MAX_LEN, 100, input_length = 100, name='embedding')\n",
        "    embedding.build(input_shape=(1,))\n",
        "    embedding.set_weights([embedding_mat])\n",
        "    model.add(embedding)\n",
        "\n",
        "    model.add(AveragePooling1D(pool_size=100))\n",
        "    print(model.output_shape)\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(50, activation='relu'))\n",
        "    model.add(Dense(10, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "\n",
        "    adam = Adam(lr = 0.001, beta_1 = 0.99, beta_2 = 0.999, epsilon = None, amsgrad = False)\n",
        "\n",
        "    model.compile(loss = 'mse', optimizer= 'adam', metrics=['mae'])\n",
        "\n",
        "    model.summary()\n",
        "\n",
        "    print('Modelo compilado...')\n",
        "\n",
        "    es = EarlyStopping(monitor='val_mae', mode='min', verbose=1, patience=10, restore_best_weights=True)\n",
        "\n",
        "    model_history = model.fit(x_train, train_y,\n",
        "              batch_size= 128,\n",
        "              epochs=30, callbacks=[es],\n",
        "              validation_data=(x_test, test_y))\n",
        "\n",
        "    y_pred = model.predict(x_test, batch_size=None, verbose=0, steps=None)\n",
        "    x_pred = model.predict(x_train, batch_size=None, verbose=0, steps=None)\n",
        "\n",
        "\n",
        "    #Metricas\n",
        "    print(\"\\n\")\n",
        "    mae = mean_absolute_error(test_y, y_pred)\n",
        "    vetMAE.append(mae)\n",
        "    print(\"MAE: %f\" % (mae))\n",
        "    medAE = median_absolute_error(test_y, y_pred)\n",
        "    vetMdae.append(medAE)\n",
        "    print(\"MedAE: %f\" % (medAE))\n",
        "    r2 = r2_score(test_y, y_pred, multioutput='raw_values')\n",
        "    vetR2.append(r2)\n",
        "    print(\"r2: %f\" % (r2))\n",
        "    mse = mean_squared_error(test_y, y_pred)\n",
        "    vetMSE.append(mse)\n",
        "    print(\"MSE: %f\" % (mse))\n",
        "    mErr = max_error(test_y, y_pred)\n",
        "    print(\"maxrror: %f\" % (mErr))\n",
        "\n",
        "    i = i + 1\n",
        "    print(\"Final \" + str(i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gRCN6dleIUV9",
        "outputId": "4ffc55f3-b96a-41eb-8f69-802f4df36d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89802 unique tokens.\n",
            "Vocab_size: 89803\n",
            "Shape of data tensor: (20981, 100)\n",
            "Shape of data test tensor: (2332, 100)\n",
            "train_y: (20981, 1)\n",
            "test_y: (2332, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d (Average  (None, 1, 100)            0         \n",
            " Pooling1D)                                                      \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 9s 43ms/step - loss: 109.9721 - mae: 4.9103 - val_loss: 93.6617 - val_mae: 4.5882\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 100.1976 - mae: 4.8145 - val_loss: 92.8896 - val_mae: 4.7689\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 98.8646 - mae: 4.7933 - val_loss: 91.4396 - val_mae: 4.6527\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 6s 40ms/step - loss: 95.7643 - mae: 4.6615 - val_loss: 87.9411 - val_mae: 4.5642\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 88.7474 - mae: 4.4588 - val_loss: 83.0059 - val_mae: 4.4397\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 81.6633 - mae: 4.3850 - val_loss: 80.8158 - val_mae: 4.8003\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 75.5762 - mae: 4.2412 - val_loss: 79.0737 - val_mae: 4.2579\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 70.2134 - mae: 4.1293 - val_loss: 76.6800 - val_mae: 4.2471\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 65.2898 - mae: 3.9608 - val_loss: 76.0121 - val_mae: 4.8772\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 60.3182 - mae: 3.8090 - val_loss: 73.2114 - val_mae: 4.4542\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 55.0177 - mae: 3.6242 - val_loss: 70.8885 - val_mae: 4.3779\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 50.2650 - mae: 3.4733 - val_loss: 69.9496 - val_mae: 4.0835\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 45.0961 - mae: 3.2574 - val_loss: 70.5378 - val_mae: 4.3594\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 41.1314 - mae: 3.1660 - val_loss: 69.1720 - val_mae: 3.9754\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 37.5597 - mae: 3.0163 - val_loss: 69.1339 - val_mae: 4.3300\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 34.3357 - mae: 2.9192 - val_loss: 68.5114 - val_mae: 4.0132\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 31.3853 - mae: 2.7873 - val_loss: 69.3641 - val_mae: 3.9961\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 5s 31ms/step - loss: 28.8928 - mae: 2.6835 - val_loss: 71.3124 - val_mae: 4.1100\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 26.9534 - mae: 2.6111 - val_loss: 70.7803 - val_mae: 4.2303\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 24.8660 - mae: 2.5195 - val_loss: 70.7075 - val_mae: 3.9880\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 23.0778 - mae: 2.4347 - val_loss: 72.2726 - val_mae: 4.2312\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 21.9202 - mae: 2.3897 - val_loss: 72.9995 - val_mae: 4.1554\n",
            "Epoch 23/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 20.3646 - mae: 2.3303 - val_loss: 75.3072 - val_mae: 4.3200\n",
            "Epoch 24/30\n",
            "163/164 [============================>.] - ETA: 0s - loss: 19.0749 - mae: 2.2646Restoring model weights from the end of the best epoch: 14.\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 19.2582 - mae: 2.2700 - val_loss: 73.2964 - val_mae: 4.2693\n",
            "Epoch 24: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.975376\n",
            "MedAE: 2.061352\n",
            "r2: 0.262390\n",
            "MSE: 69.172014\n",
            "maxrror: 92.690362\n",
            "Final 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89519 unique tokens.\n",
            "Vocab_size: 89520\n",
            "Shape of data tensor: (20981, 100)\n",
            "Shape of data test tensor: (2332, 100)\n",
            "train_y: (20981, 1)\n",
            "test_y: (2332, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_1 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_1 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 41ms/step - loss: 103.9911 - mae: 4.8664 - val_loss: 106.0261 - val_mae: 4.8653\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 98.1832 - mae: 4.7797 - val_loss: 104.2132 - val_mae: 4.7362\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 94.7374 - mae: 4.6407 - val_loss: 98.6620 - val_mae: 4.8685\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 86.7528 - mae: 4.4484 - val_loss: 91.6515 - val_mae: 4.6187\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 78.1052 - mae: 4.2364 - val_loss: 86.4625 - val_mae: 4.4388\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 69.3436 - mae: 3.9478 - val_loss: 83.3961 - val_mae: 4.3463\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 61.2195 - mae: 3.7509 - val_loss: 81.5986 - val_mae: 4.1918\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 54.2596 - mae: 3.5965 - val_loss: 83.1180 - val_mae: 4.5001\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 48.8510 - mae: 3.4590 - val_loss: 82.3226 - val_mae: 4.3608\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 43.3270 - mae: 3.3088 - val_loss: 83.5347 - val_mae: 4.3085\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 39.0973 - mae: 3.1881 - val_loss: 88.9037 - val_mae: 4.4333\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 35.2770 - mae: 3.0451 - val_loss: 89.0543 - val_mae: 4.1723\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 31.8747 - mae: 2.9423 - val_loss: 92.6675 - val_mae: 4.2400\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 29.1827 - mae: 2.8700 - val_loss: 99.4218 - val_mae: 4.3906\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 26.9209 - mae: 2.7839 - val_loss: 103.0307 - val_mae: 4.3110\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 24.7502 - mae: 2.7097 - val_loss: 108.6895 - val_mae: 4.4064\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 22.9146 - mae: 2.6342 - val_loss: 114.4215 - val_mae: 4.2475\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 21.5535 - mae: 2.5813 - val_loss: 119.2395 - val_mae: 4.2941\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 9s 56ms/step - loss: 20.8962 - mae: 2.5583 - val_loss: 127.6313 - val_mae: 4.3810\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 19.1598 - mae: 2.4797 - val_loss: 125.8192 - val_mae: 4.3399\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 18.6501 - mae: 2.4405 - val_loss: 128.8870 - val_mae: 4.2195\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 17.3247 - mae: 2.3675Restoring model weights from the end of the best epoch: 12.\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 17.3247 - mae: 2.3675 - val_loss: 129.6904 - val_mae: 4.4771\n",
            "Epoch 22: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.172268\n",
            "MedAE: 2.169291\n",
            "r2: 0.164805\n",
            "MSE: 89.054338\n",
            "maxrror: 144.911972\n",
            "Final 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90065 unique tokens.\n",
            "Vocab_size: 90066\n",
            "Shape of data tensor: (20981, 100)\n",
            "Shape of data test tensor: (2332, 100)\n",
            "train_y: (20981, 1)\n",
            "test_y: (2332, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_2 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 10)                510       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 41ms/step - loss: 107.3684 - mae: 4.9056 - val_loss: 99.0307 - val_mae: 4.7480\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 99.5945 - mae: 4.7918 - val_loss: 97.9823 - val_mae: 4.7604\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 98.2186 - mae: 4.7678 - val_loss: 96.5048 - val_mae: 4.9860\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 95.4588 - mae: 4.6578 - val_loss: 92.1580 - val_mae: 4.5212\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 89.0873 - mae: 4.4680 - val_loss: 84.8794 - val_mae: 4.3490\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 80.2847 - mae: 4.2725 - val_loss: 78.6164 - val_mae: 4.3684\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 6s 40ms/step - loss: 72.6160 - mae: 4.1629 - val_loss: 75.0145 - val_mae: 4.3032\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 66.2507 - mae: 4.0279 - val_loss: 73.5026 - val_mae: 4.2175\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 59.9389 - mae: 3.8551 - val_loss: 70.8091 - val_mae: 4.3920\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 55.1200 - mae: 3.7799 - val_loss: 69.6553 - val_mae: 4.3767\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 6s 38ms/step - loss: 50.1391 - mae: 3.6400 - val_loss: 69.8454 - val_mae: 4.3254\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 46.3445 - mae: 3.5370 - val_loss: 68.8948 - val_mae: 4.4824\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 42.3285 - mae: 3.4081 - val_loss: 68.5911 - val_mae: 4.4898\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 38.9411 - mae: 3.2803 - val_loss: 68.0618 - val_mae: 4.3055\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 36.4738 - mae: 3.1984 - val_loss: 72.5435 - val_mae: 4.9107\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 33.6139 - mae: 3.1141 - val_loss: 68.3161 - val_mae: 4.2691\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 31.7467 - mae: 3.0255 - val_loss: 69.0389 - val_mae: 4.1632\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 29.4533 - mae: 2.9336 - val_loss: 68.1143 - val_mae: 4.3420\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 6s 38ms/step - loss: 27.7745 - mae: 2.8796 - val_loss: 68.7039 - val_mae: 4.4118\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 26.3280 - mae: 2.8096 - val_loss: 68.4262 - val_mae: 4.3583\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 24.6719 - mae: 2.7266 - val_loss: 70.4855 - val_mae: 4.5216\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 23.0718 - mae: 2.6385 - val_loss: 70.1349 - val_mae: 4.4494\n",
            "Epoch 23/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 22.0165 - mae: 2.5832 - val_loss: 71.3081 - val_mae: 4.4655\n",
            "Epoch 24/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 20.9299 - mae: 2.5401 - val_loss: 72.5527 - val_mae: 4.5483\n",
            "Epoch 25/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 19.6745 - mae: 2.4651 - val_loss: 72.5285 - val_mae: 4.5007\n",
            "Epoch 26/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 19.0192 - mae: 2.4329 - val_loss: 73.8120 - val_mae: 4.5591\n",
            "Epoch 27/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 18.3396 - mae: 2.3929Restoring model weights from the end of the best epoch: 17.\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 18.3396 - mae: 2.3929 - val_loss: 71.2213 - val_mae: 4.3113\n",
            "Epoch 27: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.163181\n",
            "MedAE: 2.403390\n",
            "r2: 0.304214\n",
            "MSE: 69.038943\n",
            "maxrror: 87.634663\n",
            "Final 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 90531 unique tokens.\n",
            "Vocab_size: 90532\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_3 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 7s 39ms/step - loss: 103.9566 - mae: 4.8135 - val_loss: 108.7833 - val_mae: 4.9651\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 97.9591 - mae: 4.7586 - val_loss: 107.5432 - val_mae: 4.8487\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 5s 32ms/step - loss: 95.4079 - mae: 4.6571 - val_loss: 103.3162 - val_mae: 4.9870\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 88.5639 - mae: 4.4349 - val_loss: 95.3796 - val_mae: 4.5727\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 78.3373 - mae: 4.1938 - val_loss: 87.2029 - val_mae: 4.4815\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 69.1223 - mae: 3.9292 - val_loss: 85.4056 - val_mae: 5.2260\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 60.8195 - mae: 3.7376 - val_loss: 84.8566 - val_mae: 4.0850\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 53.0359 - mae: 3.5297 - val_loss: 77.3419 - val_mae: 4.1463\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 46.4945 - mae: 3.3664 - val_loss: 76.5588 - val_mae: 4.3659\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 5s 33ms/step - loss: 41.1482 - mae: 3.2296 - val_loss: 77.3867 - val_mae: 4.4177\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 37.1282 - mae: 3.1285 - val_loss: 78.0402 - val_mae: 4.4015\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 33.2477 - mae: 3.0127 - val_loss: 80.1635 - val_mae: 4.1037\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 6s 38ms/step - loss: 30.2652 - mae: 2.9201 - val_loss: 86.0845 - val_mae: 4.6740\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 27.7921 - mae: 2.8383 - val_loss: 80.8102 - val_mae: 4.2964\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 25.5048 - mae: 2.7515 - val_loss: 82.2589 - val_mae: 4.1546\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 23.7557 - mae: 2.6767 - val_loss: 82.2002 - val_mae: 4.3255\n",
            "Epoch 17/30\n",
            "163/164 [============================>.] - ETA: 0s - loss: 21.7370 - mae: 2.5994Restoring model weights from the end of the best epoch: 7.\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 21.6687 - mae: 2.5977 - val_loss: 85.0978 - val_mae: 4.3255\n",
            "Epoch 17: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.084952\n",
            "MedAE: 1.883314\n",
            "r2: 0.222153\n",
            "MSE: 84.856574\n",
            "maxrror: 96.576785\n",
            "Final 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89717 unique tokens.\n",
            "Vocab_size: 89718\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_4 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_14 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 7s 39ms/step - loss: 103.4448 - mae: 4.8126 - val_loss: 95.1465 - val_mae: 4.7257\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 99.4307 - mae: 4.8012 - val_loss: 93.6060 - val_mae: 4.6808\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 95.7730 - mae: 4.6705 - val_loss: 89.3769 - val_mae: 4.8065\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 87.4714 - mae: 4.4509 - val_loss: 85.5445 - val_mae: 4.9975\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 77.1127 - mae: 4.1949 - val_loss: 80.0951 - val_mae: 4.4652\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 6s 38ms/step - loss: 67.2943 - mae: 3.8944 - val_loss: 78.6187 - val_mae: 4.4397\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 5s 32ms/step - loss: 58.9965 - mae: 3.6920 - val_loss: 78.1553 - val_mae: 4.0572\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 51.7144 - mae: 3.5033 - val_loss: 77.2494 - val_mae: 4.1162\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 45.9571 - mae: 3.3821 - val_loss: 77.5818 - val_mae: 4.1062\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 40.3814 - mae: 3.2221 - val_loss: 79.2316 - val_mae: 4.0473\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 5s 33ms/step - loss: 36.6218 - mae: 3.1391 - val_loss: 86.0218 - val_mae: 4.6937\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 5s 33ms/step - loss: 32.9393 - mae: 3.0338 - val_loss: 84.0779 - val_mae: 4.3150\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 30.8650 - mae: 2.9506 - val_loss: 82.6971 - val_mae: 4.0687\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 27.8925 - mae: 2.8540 - val_loss: 85.2765 - val_mae: 4.2315\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 25.5781 - mae: 2.7624 - val_loss: 87.9558 - val_mae: 4.3291\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 23.9565 - mae: 2.7060 - val_loss: 86.7905 - val_mae: 4.0942\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 22.2676 - mae: 2.6301 - val_loss: 90.0336 - val_mae: 4.2136\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 20.5246 - mae: 2.5580 - val_loss: 93.5450 - val_mae: 4.3592\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 19.4930 - mae: 2.5086 - val_loss: 95.4273 - val_mae: 4.3166\n",
            "Epoch 20/30\n",
            "163/164 [============================>.] - ETA: 0s - loss: 18.3822 - mae: 2.4528Restoring model weights from the end of the best epoch: 10.\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 18.3675 - mae: 2.4524 - val_loss: 98.3459 - val_mae: 4.4274\n",
            "Epoch 20: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.047320\n",
            "MedAE: 2.114678\n",
            "r2: 0.172141\n",
            "MSE: 79.231618\n",
            "maxrror: 111.327484\n",
            "Final 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89516 unique tokens.\n",
            "Vocab_size: 89517\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_5 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 135.9078 - mae: 6.1255 - val_loss: 155.2630 - val_mae: 6.2661\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 133.8277 - mae: 5.9529 - val_loss: 153.2947 - val_mae: 6.1071\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 131.9660 - mae: 5.7942 - val_loss: 151.3922 - val_mae: 5.9493\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 130.1779 - mae: 5.6373 - val_loss: 149.5669 - val_mae: 5.7938\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 6s 38ms/step - loss: 128.4611 - mae: 5.4831 - val_loss: 147.8124 - val_mae: 5.6404\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 126.8116 - mae: 5.3302 - val_loss: 146.1272 - val_mae: 5.4890\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 125.2300 - mae: 5.1882 - val_loss: 144.5018 - val_mae: 5.3716\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 123.7102 - mae: 5.0891 - val_loss: 142.9495 - val_mae: 5.2784\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 6s 34ms/step - loss: 122.2486 - mae: 4.9958 - val_loss: 141.4477 - val_mae: 5.1857\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 120.8437 - mae: 4.9031 - val_loss: 140.0010 - val_mae: 5.0938\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 119.4945 - mae: 4.8107 - val_loss: 138.6175 - val_mae: 5.0032\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 118.2011 - mae: 4.7192 - val_loss: 137.2816 - val_mae: 4.9131\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 7s 46ms/step - loss: 116.9627 - mae: 4.6285 - val_loss: 135.9978 - val_mae: 4.8238\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 115.7751 - mae: 4.5488 - val_loss: 134.7752 - val_mae: 4.7638\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 114.6391 - mae: 4.4983 - val_loss: 133.6019 - val_mae: 4.7172\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 113.5526 - mae: 4.4511 - val_loss: 132.4742 - val_mae: 4.6710\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 112.5146 - mae: 4.4043 - val_loss: 131.3979 - val_mae: 4.6254\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 111.5232 - mae: 4.3583 - val_loss: 130.3645 - val_mae: 4.5800\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 110.5775 - mae: 4.3121 - val_loss: 129.3799 - val_mae: 4.5353\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 109.6768 - mae: 4.2668 - val_loss: 128.4419 - val_mae: 4.4911\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 108.8210 - mae: 4.2227 - val_loss: 127.5437 - val_mae: 4.4573\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 108.0080 - mae: 4.2085 - val_loss: 126.6987 - val_mae: 4.4572\n",
            "Epoch 23/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 107.2371 - mae: 4.2077 - val_loss: 125.8833 - val_mae: 4.4572\n",
            "Epoch 24/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 106.5066 - mae: 4.2077 - val_loss: 125.1177 - val_mae: 4.4571\n",
            "Epoch 25/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 105.8171 - mae: 4.2069 - val_loss: 124.3862 - val_mae: 4.4571\n",
            "Epoch 26/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 105.1659 - mae: 4.2061 - val_loss: 123.7041 - val_mae: 4.4570\n",
            "Epoch 27/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 104.5522 - mae: 4.2056 - val_loss: 123.0532 - val_mae: 4.4570\n",
            "Epoch 28/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 103.9752 - mae: 4.2053 - val_loss: 122.4395 - val_mae: 4.4569\n",
            "Epoch 29/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 103.4354 - mae: 4.2040 - val_loss: 121.8602 - val_mae: 4.4573\n",
            "Epoch 30/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 102.9311 - mae: 4.2082 - val_loss: 121.3246 - val_mae: 4.4653\n",
            "\n",
            "\n",
            "MAE: 4.465269\n",
            "MedAE: 2.119888\n",
            "r2: -0.045915\n",
            "MSE: 121.324587\n",
            "maxrror: 95.880112\n",
            "Final 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89090 unique tokens.\n",
            "Vocab_size: 89091\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_6 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_20 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 10s 55ms/step - loss: 124.1468 - mae: 5.5122 - val_loss: 109.5469 - val_mae: 5.0676\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 98.8689 - mae: 4.7661 - val_loss: 108.7302 - val_mae: 5.0323\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 97.5521 - mae: 4.7539 - val_loss: 107.7659 - val_mae: 4.9267\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 95.6979 - mae: 4.7016 - val_loss: 106.0095 - val_mae: 4.9273\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 92.1648 - mae: 4.5524 - val_loss: 102.7814 - val_mae: 4.8729\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 85.6887 - mae: 4.3857 - val_loss: 99.0759 - val_mae: 4.7942\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 77.8059 - mae: 4.1735 - val_loss: 95.4408 - val_mae: 4.5067\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 69.8584 - mae: 3.9388 - val_loss: 91.8611 - val_mae: 4.5775\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 9s 52ms/step - loss: 63.0726 - mae: 3.7722 - val_loss: 90.9120 - val_mae: 4.4909\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 57.0929 - mae: 3.5964 - val_loss: 90.2553 - val_mae: 4.2494\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 51.6555 - mae: 3.4388 - val_loss: 88.3569 - val_mae: 4.3462\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 46.8051 - mae: 3.2975 - val_loss: 88.1560 - val_mae: 4.2521\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 42.4727 - mae: 3.1540 - val_loss: 91.8687 - val_mae: 4.8028\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 38.6863 - mae: 3.0372 - val_loss: 88.2445 - val_mae: 4.4639\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 35.0907 - mae: 2.8999 - val_loss: 89.3781 - val_mae: 4.5111\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 32.5504 - mae: 2.8295 - val_loss: 88.8521 - val_mae: 4.3148\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 29.6411 - mae: 2.7110 - val_loss: 89.9814 - val_mae: 4.3748\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 27.8235 - mae: 2.6554 - val_loss: 90.7329 - val_mae: 4.2901\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 8s 50ms/step - loss: 25.3163 - mae: 2.5200 - val_loss: 91.2679 - val_mae: 4.2855\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 24.0235 - mae: 2.4920Restoring model weights from the end of the best epoch: 10.\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 24.0235 - mae: 2.4920 - val_loss: 93.8797 - val_mae: 4.2699\n",
            "Epoch 20: early stopping\n",
            "\n",
            "\n",
            "MAE: 4.249420\n",
            "MedAE: 2.188260\n",
            "r2: 0.176927\n",
            "MSE: 90.255307\n",
            "maxrror: 94.620683\n",
            "Final 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89905 unique tokens.\n",
            "Vocab_size: 89906\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_7 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_22 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 42ms/step - loss: 104.8127 - mae: 4.7693 - val_loss: 97.2341 - val_mae: 4.8422\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 99.5512 - mae: 4.8218 - val_loss: 96.1964 - val_mae: 4.7426\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 97.7935 - mae: 4.7493 - val_loss: 94.1028 - val_mae: 4.5601\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 93.2152 - mae: 4.5709 - val_loss: 88.9373 - val_mae: 4.5568\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 84.8818 - mae: 4.3571 - val_loss: 83.6467 - val_mae: 4.5112\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 75.9295 - mae: 4.1167 - val_loss: 79.3090 - val_mae: 4.0358\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 67.7297 - mae: 3.8881 - val_loss: 76.3526 - val_mae: 4.1960\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 60.4722 - mae: 3.7105 - val_loss: 74.5372 - val_mae: 4.2589\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 54.2498 - mae: 3.5711 - val_loss: 74.0439 - val_mae: 4.0241\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 48.1364 - mae: 3.3938 - val_loss: 73.4894 - val_mae: 3.8697\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 43.1970 - mae: 3.2551 - val_loss: 73.9037 - val_mae: 4.1326\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 38.9062 - mae: 3.1451 - val_loss: 77.8380 - val_mae: 4.3210\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 9s 53ms/step - loss: 34.8564 - mae: 3.0104 - val_loss: 75.6154 - val_mae: 3.8837\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 31.5744 - mae: 2.9011 - val_loss: 77.4798 - val_mae: 4.0341\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 9s 54ms/step - loss: 28.7883 - mae: 2.8072 - val_loss: 77.3710 - val_mae: 3.8549\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 26.4343 - mae: 2.7071 - val_loss: 80.5294 - val_mae: 4.0495\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 24.2900 - mae: 2.6121 - val_loss: 82.9216 - val_mae: 4.2845\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 8s 51ms/step - loss: 22.5928 - mae: 2.5629 - val_loss: 84.8818 - val_mae: 4.1237\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 11s 69ms/step - loss: 20.8829 - mae: 2.4671 - val_loss: 86.1393 - val_mae: 4.0679\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 19.6168 - mae: 2.4067 - val_loss: 85.9990 - val_mae: 4.0644\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 18.5656 - mae: 2.3643 - val_loss: 85.1771 - val_mae: 4.0224\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 17.1044 - mae: 2.2696 - val_loss: 89.6144 - val_mae: 4.1241\n",
            "Epoch 23/30\n",
            "164/164 [==============================] - 7s 40ms/step - loss: 16.5586 - mae: 2.2509 - val_loss: 88.0701 - val_mae: 4.2625\n",
            "Epoch 24/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 15.3107 - mae: 2.1798 - val_loss: 88.2105 - val_mae: 4.1548\n",
            "Epoch 25/30\n",
            "163/164 [============================>.] - ETA: 0s - loss: 14.3499 - mae: 2.1191Restoring model weights from the end of the best epoch: 15.\n",
            "164/164 [==============================] - 6s 35ms/step - loss: 14.3529 - mae: 2.1184 - val_loss: 94.3317 - val_mae: 4.2620\n",
            "Epoch 25: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.854906\n",
            "MedAE: 1.930856\n",
            "r2: 0.207174\n",
            "MSE: 77.370999\n",
            "maxrror: 97.067646\n",
            "Final 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89590 unique tokens.\n",
            "Vocab_size: 89591\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_8 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 12s 65ms/step - loss: 105.8914 - mae: 4.9083 - val_loss: 78.5646 - val_mae: 4.6101\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 9s 55ms/step - loss: 101.2153 - mae: 4.8194 - val_loss: 77.3490 - val_mae: 4.5470\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 98.0903 - mae: 4.7336 - val_loss: 74.0360 - val_mae: 4.0780\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 89.7895 - mae: 4.4723 - val_loss: 69.5674 - val_mae: 4.5896\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 78.7962 - mae: 4.1965 - val_loss: 65.1550 - val_mae: 3.9205\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 69.4648 - mae: 3.9635 - val_loss: 62.9882 - val_mae: 4.0206\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 6s 38ms/step - loss: 60.9549 - mae: 3.7375 - val_loss: 61.3719 - val_mae: 3.7431\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 53.9327 - mae: 3.5673 - val_loss: 66.1937 - val_mae: 4.3742\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 47.5152 - mae: 3.3948 - val_loss: 65.7389 - val_mae: 4.2556\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 41.9653 - mae: 3.2512 - val_loss: 62.4540 - val_mae: 3.8233\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 6s 38ms/step - loss: 37.9061 - mae: 3.1462 - val_loss: 64.5940 - val_mae: 3.9692\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 7s 41ms/step - loss: 34.2193 - mae: 3.0464 - val_loss: 65.4849 - val_mae: 3.9613\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 30.9170 - mae: 2.9217 - val_loss: 64.9971 - val_mae: 3.8584\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 52ms/step - loss: 28.9004 - mae: 2.8881 - val_loss: 67.2925 - val_mae: 3.9072\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 6s 36ms/step - loss: 26.6617 - mae: 2.7801 - val_loss: 67.2722 - val_mae: 3.7726\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 24.2484 - mae: 2.7007 - val_loss: 71.1142 - val_mae: 3.9450\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 22.9238 - mae: 2.6299Restoring model weights from the end of the best epoch: 7.\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 22.9238 - mae: 2.6299 - val_loss: 71.0312 - val_mae: 3.8985\n",
            "Epoch 17: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.743106\n",
            "MedAE: 2.076404\n",
            "r2: 0.222295\n",
            "MSE: 61.371918\n",
            "maxrror: 94.392043\n",
            "Final 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 89471 unique tokens.\n",
            "Vocab_size: 89472\n",
            "Shape of data tensor: (20982, 100)\n",
            "Shape of data test tensor: (2331, 100)\n",
            "train_y: (20982, 1)\n",
            "test_y: (2331, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(None, 1, 100)\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 100, 100)          2331300   \n",
            "                                                                 \n",
            " average_pooling1d_9 (Avera  (None, 1, 100)            0         \n",
            " gePooling1D)                                                    \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 100)               0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 50)                5050      \n",
            "                                                                 \n",
            " dense_28 (Dense)            (None, 10)                510       \n",
            "                                                                 \n",
            " dense_29 (Dense)            (None, 1)                 11        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2336871 (8.91 MB)\n",
            "Trainable params: 2336871 (8.91 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Modelo compilado...\n",
            "Epoch 1/30\n",
            "164/164 [==============================] - 8s 44ms/step - loss: 109.2505 - mae: 4.9730 - val_loss: 94.4534 - val_mae: 4.6527\n",
            "Epoch 2/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 99.7962 - mae: 4.8218 - val_loss: 93.7558 - val_mae: 4.7478\n",
            "Epoch 3/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 97.8665 - mae: 4.7653 - val_loss: 91.8216 - val_mae: 4.6701\n",
            "Epoch 4/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 91.9547 - mae: 4.5266 - val_loss: 87.4656 - val_mae: 4.4797\n",
            "Epoch 5/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 81.9889 - mae: 4.3039 - val_loss: 86.3425 - val_mae: 3.9558\n",
            "Epoch 6/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 73.4740 - mae: 4.0955 - val_loss: 80.1880 - val_mae: 4.2416\n",
            "Epoch 7/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 65.1325 - mae: 3.8502 - val_loss: 80.2581 - val_mae: 3.9368\n",
            "Epoch 8/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 58.9053 - mae: 3.7103 - val_loss: 76.9383 - val_mae: 4.0055\n",
            "Epoch 9/30\n",
            "164/164 [==============================] - 6s 39ms/step - loss: 52.1890 - mae: 3.5233 - val_loss: 76.5268 - val_mae: 4.2819\n",
            "Epoch 10/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 47.1048 - mae: 3.4353 - val_loss: 74.9464 - val_mae: 4.0447\n",
            "Epoch 11/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 42.1670 - mae: 3.2739 - val_loss: 74.4543 - val_mae: 4.0483\n",
            "Epoch 12/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 38.3556 - mae: 3.1666 - val_loss: 76.1448 - val_mae: 4.2511\n",
            "Epoch 13/30\n",
            "164/164 [==============================] - 7s 44ms/step - loss: 35.1293 - mae: 3.1038 - val_loss: 73.4477 - val_mae: 3.9256\n",
            "Epoch 14/30\n",
            "164/164 [==============================] - 8s 47ms/step - loss: 32.4827 - mae: 3.0216 - val_loss: 74.3393 - val_mae: 4.0205\n",
            "Epoch 15/30\n",
            "164/164 [==============================] - 7s 45ms/step - loss: 29.7630 - mae: 2.9114 - val_loss: 74.5129 - val_mae: 3.9486\n",
            "Epoch 16/30\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 27.0310 - mae: 2.7748 - val_loss: 75.4582 - val_mae: 4.0614\n",
            "Epoch 17/30\n",
            "164/164 [==============================] - 8s 46ms/step - loss: 25.4558 - mae: 2.7393 - val_loss: 79.3169 - val_mae: 4.4879\n",
            "Epoch 18/30\n",
            "164/164 [==============================] - 6s 37ms/step - loss: 23.8402 - mae: 2.6938 - val_loss: 75.8449 - val_mae: 4.0392\n",
            "Epoch 19/30\n",
            "164/164 [==============================] - 7s 42ms/step - loss: 21.9803 - mae: 2.5717 - val_loss: 77.7952 - val_mae: 4.0774\n",
            "Epoch 20/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 20.9639 - mae: 2.5422 - val_loss: 77.1149 - val_mae: 3.9589\n",
            "Epoch 21/30\n",
            "164/164 [==============================] - 8s 48ms/step - loss: 19.4013 - mae: 2.4603 - val_loss: 80.4522 - val_mae: 4.1899\n",
            "Epoch 22/30\n",
            "164/164 [==============================] - 7s 43ms/step - loss: 18.7220 - mae: 2.4344 - val_loss: 79.3996 - val_mae: 4.1161\n",
            "Epoch 23/30\n",
            "164/164 [==============================] - ETA: 0s - loss: 17.4551 - mae: 2.3609Restoring model weights from the end of the best epoch: 13.\n",
            "164/164 [==============================] - 8s 49ms/step - loss: 17.4551 - mae: 2.3609 - val_loss: 83.1618 - val_mae: 4.3977\n",
            "Epoch 23: early stopping\n",
            "\n",
            "\n",
            "MAE: 3.925605\n",
            "MedAE: 2.117419\n",
            "r2: 0.224098\n",
            "MSE: 73.447655\n",
            "maxrror: 95.488490\n",
            "Final 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-a8e0a612ed1b>:79: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(\"r2: %f\" % (r2))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "maeMedio = np.mean(vetMAE)\n",
        "madAEMedio = np.mean(vetMdae)\n",
        "r2Medio = np.mean(vetR2)\n",
        "mseMedio = np.mean(vetMSE)\n",
        "stdMae = np.std(vetMAE)\n",
        "stdr2 = np.std(vetR2)\n",
        "stdMse = np.std(vetMSE)\n",
        "\n",
        "print('maeMedio: ' + str(maeMedio))\n",
        "print('madAEMedio: ' + str(madAEMedio))\n",
        "print('r2Medio: ' + str(r2Medio))\n",
        "print('mseMedio: ' + str(mseMedio))\n",
        "print('stdMae: ' + str(stdMae))\n",
        "print('stdr2: ' + str(stdr2))\n",
        "print('stdMse: ' + str(stdMse))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naaglC_Jazuw",
        "outputId": "63fb9403-2ef4-4e97-b422-bba92a2872cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "maeMedio: 4.068140302084701\n",
            "madAEMedio: 2.1064851105213167\n",
            "r2Medio: 0.19102833519200701\n",
            "mseMedio: 81.51239518213701\n",
            "stdMae: 0.19778837641387265\n",
            "stdr2: 0.08865074239612636\n",
            "stdMse: 15.90728631573578\n"
          ]
        }
      ]
    }
  ]
}